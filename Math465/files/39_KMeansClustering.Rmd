---
title: "K Means Clustering"
output:
  rmdformats::html_clean
---

```{r setup}
library(tidymodels)
tidymodels_prefer()
library(readr)
library(janitor)
```

# Intro to Clustering

To get the idea of clustering, we're going to create some artificial data and perform clustering on this. You can see that we've created three distinct clusters in a two variable space. This should be fairly simple for k-means clustering to identify.

```{r}
set.seed(465)

centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)

labelled_points <- 
  centers |>
  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm)
  ) |> 
  select(-num_points) |> 
  unnest(cols = c(x1, x2))

ggplot(labelled_points, aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.3)
```

## K Means using `kmeans`

The k-means model specification is in the `tidyclust` library. To specify a k-means model in tidymodels, simply choose a value of num_clusters:

```{r}
library(tidyclust)

kmeans_spec <- k_means(num_clusters = 3)
kmeans_spec
```

```{r}
# note that you don't need to provide the outcome variable, because there isn't one!
kmeans_rec <- recipe(~., data=labelled_points) |>
  #we don't want to use the cluster variable, but I'm going to use it later so just update the role
  update_role(cluster, new_role="label") |> 
  # k means uses distances, so we'll normalize the predictors
  step_normalize(all_numeric())

kmeans_wf <- workflow() |>
  add_model(kmeans_spec) |>
  add_recipe(kmeans_rec)
```

```{r}
set.seed(465)
kmeans_fit <- kmeans_wf |> fit(data=labelled_points)
kmeans_fit
```


The model output is hard to read, but gives the centers of each cluster, and the variation within each cluster (what we are trying to minimize). To get a nicer view, we can use `tidy()` and `glance()`:

```{r}
tidy(kmeans_fit)
glance(kmeans_fit)
```

To get the predicted labels, we can use the augment function. I'll also plot them to see how the algorithm did.

```{r}
# Try changing the number of clusters to see what happens!
# kmeans_fit <- workflow() |>
#   add_model(
#     k_means(num_clusters = 2)
#     ) |>
#   add_recipe(kmeans_rec) |>
#   fit(data=labelled_points)
clustered_points <- kmeans_fit |> augment(labelled_points)
clustered_points

plot <- clustered_points |> 
  ggplot(aes(x1, x2)) + 
  geom_point(aes(color=.pred_cluster, shape=cluster), alpha=.5) 
plot
```



## Choosing the number of clusters

We generated simulated data that had 3 pre-defined clusters. But if we didn't know how many clusters there were, how could we choose between them? Typically, we try clustering for many different Ks, plot their within group sum of squares, and choose one based on the "elbow method". We look for where the plot of SSW flattens out, i.e., the elbow.

```{r}
set.seed(465)
ssw <- c()
# perform clustering for many different k's:
for (k in 1:8) {
  kmeans_fit <- workflow() |>
    add_model(
    k_means(num_clusters = k)
    ) |>
    add_recipe(kmeans_rec) |> 
    fit(data=labelled_points)
  ssw <- c(ssw, glance(kmeans_fit)$tot.withinss) # get SSW
} 

ggplot(data=NULL, aes(y=ssw, x=1:8)) + 
  geom_line() + geom_point() +
  labs(x="K: number of clusters", y="Total Within Group SS")
```

As k changes from 1 to 2 and 2 to 3, the sum of squares decreases a lot. It decreases much less after that, and hence flattens out after about 3 or 4 clusters. This is not an exact science. Remember, we typically won't know how many clusters there are supposed to be!

# NBA Example Background and EDA

Basketball teams traditionally assign players to 5 positions: Center, Forward, Small Forward, Shooting Guard and Point Guard. But anyone who watches basketball today knows that these traditional positions do not mean much anymore. Here we'll look at the stats of different players from the most recent season and see if we can put them into position groups based on role in the team.

This data is from [Basketball Reference](https://www.basketball-reference.com/leagues/NBA_2023_per_minute.html). All data is per 36 minutes, which is the length of a game.

```{r}
# I need to clean the names since variable names can't start with a number. 
# clean_names adds an x before those
nba <- read_csv("nba22-23.csv") |> clean_names()
```

I want to exclude players who didn't play much this season, as their stats might skew the results. Looking at the histogram of minutes played, I can see that there are a big chunk of players that played 250 min or less. I'll arbitrarily choose 360 minutes---the equivalent of 10 full games played---as my cutoff. We should have 393 players remaining.

```{r}
nba |> ggplot(aes(x=mp)) + geom_histogram(bins=20)
```

```{r}
nba <- nba |> filter(mp >= 360)
```

Let's see how the traditional positions are distributed:

```{r}
nba |> ggplot(aes(pos)) + geom_bar()
```

It looks more or less even. Notice that there are some players with more than one category. Normally I would clean that up, but since we aren't going to use this information, I won't worry about this for now.

Traditionally, a point guard is a small player that will carry the ball up the court, get a lot of assists, steals, and potentially shoot three point shots. Let's see how that plays out here:

```{r}
nba |> ggplot(aes(ast, stl, color=pos)) + geom_point(position="jitter")
```

A center, on the other hand, is typically a tall player that will rebound, block shots, score close to the basket and take a lot of free-throws.

```{r}
nba |> ggplot(aes(blk, trb, color=pos)) + geom_point(position="jitter")
```

What about other stats? This one is a mess---there is very little separation between the traditional positions.

```{r}
# remember the x is there because variable names can't begin with numbers in R
nba |> ggplot(aes(x3pa, stl, color=pos)) + geom_point(position="jitter")
```


## Clustering based on Offensive Stats

Let's see if we can cluster players that have similar offensive statistics. We could add in other stats here, or consider defensive stats to determine overall play styles, but this is meant to be a simple example.

```{r}
nba_off <- nba |> select(player, pos, pts, ast, tov, orb, ft, x2p, x3p) 
```

First, let's see if there is a big difference in these stats based on traditional position:

```{r}
nba_off |> 
  select(-player) |>
  group_by(pos) |>
  summarize(across(everything(), mean)) |>
  filter(pos %in% c("C", "PF", "SF", "SG", "PG"))
```

There are some differences, but many of the categories are similar, e.g., scoring seems to be spread evenly between the traditional positions.

Next, we'll perform the k-means clustering. Notice again that we need to scale the variables.

```{r}
set.seed(465)
nba_rec <- recipe(~., data=nba_off) |>
  update_role(player, new_role="player") |>
  update_role(pos, new_role="position") |>
  step_normalize(all_numeric())

nba_spec <- k_means(num_clusters=6)

nba_wf <- workflow() |>
  add_model(nba_spec) |>
  add_recipe(nba_rec)

nba.clusters <- nba_wf |>
  fit(data=nba_off) |>
  augment(nba_off)
```

Let's take a look at some of the players in each cluster and explore this.

```{r}
nba.clusters |> 
  group_by(.pred_cluster) |>
  select(-c(player, pos)) |>
  summarize(cluster_size=n(), across(everything(), mean))

for(k in 1:6){
  print(
    nba.clusters |> filter(.pred_cluster == paste("Cluster", as.character(k), sep="_")) |> slice_sample(n=5)
  )
}
```


## Choosing the number of clusters

I chose 6 clusters arbitrarily. Since it's not clear exactly what the right number should be, let's try a few different values and see.

```{r}
set.seed(465)
ssw <- c()
K_max <- 12
for (k in 1:K_max) {
  nba_clusts <- workflow() |>
    add_model(
      k_means(num_clusters = k)
    ) |>
    add_recipe(nba_rec) |>
    fit(data=nba_off) 
  ssw <- c(ssw, glance(nba_clusts)$tot.withinss) # get SSW
} 

ggplot(data=NULL, aes(y=ssw, x=1:K_max)) + geom_line() +
  labs(x = "K", y = "SSE")
# To get a better picture, we can look at how much the SSW changed from the previous clustering
tibble(k=2:(K_max+1), ssw_diff = ssw - lead(ssw)) |>
  filter(k<K_max+1) |>
  ggplot(aes(k, ssw_diff)) + geom_line() +
  labs(x = "K", y = "SSE Rolling Difference", title = "A Clearer Picture")
```

To me this looks like 4-5 is the right number. And that aligns somewhat with what we saw previously. A few of the groups didn't look that different!

```{r}
set.seed(465) # for reproducibility

nba_spec <- k_means(num_clusters=4)

nba_clusters4_fit <- workflow() |>
  add_model(nba_spec) |>
  add_recipe(nba_rec) |>
  fit(data=nba_off) 

nba_clusters4 <- nba_clusters4_fit|>
  augment(nba_off)

nba_clusters4 |> 
  group_by(.pred_cluster) |>
  select(-c(player, pos))
  summarize(cluster_size=n(), across(everything(), mean))

nba_clusters4 |> filter(.pred_cluster == "Cluster_4") |> head(n=10)
```

There are a number of ways we could explore this further! This data is high-dimensional, so it's much more difficult to see the relationships than previously.

```{r}
nba_clusters4 |> ggplot(aes(pts, ast)) + geom_point()
```

If you know the NBA, take a look at the players in each cluster and see if the similarities make sense!

You can also assign new data to a cluster (based on the closest centroid)

```{r}
player1 <- tibble(
  player = "Name",
  pos = "SG",
  pts=25, ast=10, tov=1, orb=2, ft=3, x2p=4, x3p=5
)

nba_clusters4_fit |> predict(player1)
```


## Clustering with PCA

If time

```{r}
set.seed(465) # for reproducibility

pca_rec <- nba_rec |> step_pca(all_numeric())

pca_predclusters4 <- workflow() |>
  add_model(
    k_means(num_clusters=4)
  ) |>
  add_recipe(pca_rec) |>
  fit(data=nba_off) |>
  predict(nba_off)

pca_clusters4 <- bind_cols(
  bake(prep(pca_rec), new_data=NULL),
  pca_predclusters4)

pca_clusters4 |> ggplot(aes(PC1, PC2, color=.pred_cluster)) + geom_point()
```
