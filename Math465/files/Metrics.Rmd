---
title: "465 Metric Examples"
output: html_document
---

```{r setup, message=FALSE}
library(Lahman)
library(tidymodels)
library(tidyverse)
library(janitor)
tidymodels_prefer()
```


# Precision-Recall Curves

From the presentation by Luke Hamm, Andrew Howe, Noah Husted. Adapted for `tidymodels` and `yardstick` by your professor.

```{r}
# Import data
data <- Batting |> 
  filter(yearID >= 1960, AB >= 200) |> 
  mutate(HR30 = as.factor(ifelse(HR >= 30, 1, 0)))

# Create training and test sets
set.seed(12345)

split <- initial_split(data, prop = 0.8) # should be initial_time_split, but will update later

trainData <- training(split)
testData <- testing(split)
folds <- vfold_cv(trainData, v = 10, repeats = 5)


# preprocessing steps
rec <- recipe(HR30 ~ AB + H + RBI + BB + SO + SB, data = trainData) |>
  step_naomit(all_predictors()) |> # to omit missing rows
  step_normalize(all_numeric_predictors())
```

```{r}
# simple way, no tuning
model <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")

hr_wkflow <- workflow() |>
  add_model(model) |>
  add_recipe(rec)

hr_pred <- hr_wkflow |>
  fit(data = trainData) |>
  augment(new_data = testData)

# use the `pr_curve` function from `yardstick`
# the event of interest is HR30 = 1, which is second
pr_curve_info <- pr_curve(hr_pred, .pred_1, truth = HR30, event_level = "second")
autoplot(pr_curve_info) # could also use ggplot, as below

# gives the area under the PR curve, values close to 1 are better
pr_auc(hr_pred, .pred_1, truth = HR30, event_level = "second")

# ggplot(pr_curve_info, aes(x=recall, y=precision)) +
#   geom_path() +
#   coord_equal()
```


Using this model, achieving 100% precision or 100% recall would come at a significant cost to the other metric. In this case of predicting 30+ HR seasons, precision is likely more valuable than recall, making a threshold where precision is 80% and recall is 50% the most optimal balance.

We could look in the `pr_curve_info` tibble to see that we would favor a threshold of about 0.63:

```{r}
pr_curve_info |>
  filter(precision <.81, precision > .80)
```

Alternatively, the `threshold_perf` function gives us the values of a metric over different thresholds:

```{r}
library(probably)
threshold_perf(
  hr_pred, 
  truth=HR30, 
  estimate=.pred_1,
  thresholds = seq(0.2, 0.8, by = 0.1),
  metrics = metric_set(f_meas, sens, spec),
  event_level = "second"
  )
```

# Multi-class Sensitivity and Specificity 

Luke Johnsen, Jacob Turrubiartes, Joshua Winnes

```{r}
Diabetes <- read.csv("diabetes.csv") |> mutate(group = as.factor(group))
```

## Splits and Recipe

```{r}
set.seed(2023)
diab_split <- Diabetes %>%
  initial_split(
    prop = 0.7,
    strata = group
  )
diab_test <- testing(diab_split)
diab_train <- training(diab_split)

recipe <- recipe(group ~ ., data = diab_train) %>%
  step_normalize(all_predictors())
```

```{r}
library(discrim)
lda_model <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")
```

## Get Results

```{r}
lda_workflow <- workflow() %>%
  add_model(lda_model) %>%
  add_recipe(recipe)

lda_fit <- lda_workflow %>%
  fit(data= diab_train)

predictions <- predict(lda_fit, new_data = diab_test)

results <- diab_test %>%
  select(group) %>%
  bind_cols(predictions)
```

## Calculate Metrics

```{r}
sensitivity(results, group, .pred_class, estimator = "macro")
sensitivity(results, group, .pred_class, estimator = "macro_weighted")
sensitivity(results, group, .pred_class, estimator = "micro")

specificity(results, group, .pred_class, estimator = "macro")
specificity(results, group, .pred_class, estimator = "macro_weighted")
specificity(results, group, .pred_class, estimator = "micro")
```
